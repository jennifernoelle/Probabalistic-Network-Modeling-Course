---
title: "Latent Factors"
output: html_document
date: "2023-10-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# TO DO:

-   ~~Write out model~~
-   Explain HMC
-   ~~Remove X monitors if it's working~~
-   Figure out why logL is NA in trade application
-   ~~Try Hoff's package~~
-   Try elliptical slice
-   X Simplify Nimble code
-   X Simulate data and code Nimble model
-   X Add polyga gamma
-   ~~Evaluate performance~~
-   ~~Add trade application~~


```{r, echo = FALSE, message = FALSE}
## Set directories
data_path <- 'Data/'
save_path <- 'Results/'

# Load packages
library(tidyverse)
library(here)
library(igraph)
library(ggraph)
library(RColorBrewer)
library(pheatmap)
library(nimble)
library(nimbleHMC)
library(bayesplot)
library(mvtnorm)
library(gridExtra)
library(pROC)

# Do we want to save any output?
save_files <- TRUE

```
# 1. Simulate from a latent space network model 

To begin, we simulate form the latent space network model with a probit link function, 
$\Phi^{-1}$, where $\Phi$ is the cumulative distribution function of the standard normal distribution. 

As described in lecture, this model connection propensity between any two nodes to 
the inner product of their latent features. 

$p(y_{ij}=1) = \pi_{ij} = \Phi(\alpha + \sum_{h=1}^H \lambda_h \eta_{ih}\eta_{jh})$

Here, $\alpha$ is an intercept controlling the denseness of connections, 
$\eta_i, \eta_j$ are $H-dimensional$ vectors of latent features for nodes $i$ and $j$
respectively, and $\lambda_h$ is a shrinkage factoring controlling the influence
of each dimension of the latent space. 



## 1a. Visualizing a small network 

We begin by simulating a network with only 10 nodes for ease of visualization.

```{r}
set.seed(1234)

# Data (and latent space) dimensions
V <- 10 # Number of vertices
H <- 10 # Dimension of latent space

# Hyperparameters
a1 <- 2.5 # Gamma shrinkage parameter for factor 1
a2 <- 2.5 # Gamma shrinkage parameters for factors 2:H

meanP <- 0.25 # Moderately sparse network
mu0 <- probit(meanP) # Prior mean for intercept
sd.mu <- 0.1 # Prior sd for intercept: consider probit scale

# Simulate multiplicative gamma shrinkage process
U <- rep(NA,H)
U[1] <- rgamma(1, a1, 1)
U[2:H] <- rgamma(H-1, a2, 1)

Tau <- rep(NA, H)
for(h in 1:H){
  Tau[h] <- prod(U[1:h])
}

# Simulate latent factors
X <- matrix(NA, nrow = V, ncol = H)
for(h in 1:H){
  X[,h] <- rmvnorm(n = 1, mean = rep(0, V), sigma = diag(Tau[h]^(-1), nrow = V)) 
}

# Simulate intercept
mu <- rnorm(1,mean = mu0, sd = sd.mu) # Normal prior for baseline connection score

# Compute the linear predictor and Z latent connection score
M.Z <- mu + X %*% t(X) 
P <- phi(M.Z) # Gaussian CDF

# Look at the distribution of connection probabilities
hist(c(P))
round(quantile(c(P), seq(0,1, 0.1), na.rm = TRUE), 4)

# Sample edges
Y <- matrix(NA, V,V)
Y[upper.tri(Y)] <- rbinom(V*(V-1)/2, 1, P[upper.tri(P)])
Y[lower.tri(Y)] <- t(Y)[lower.tri(Y)]

diag(Y) <- diag(P) <- NA # Makes plotting more clear

rownames(Y) <- colnames(Y) <- rownames(P) <- colnames(P) <- LETTERS[1:V]

p1 <- pheatmap(P, cluster_rows = FALSE, cluster_cols = FALSE, 
         color=colorRampPalette(brewer.pal(9,"Blues")[c(1,8)])(30), 
         main = "Probability Matrix")[[4]]

p2 <- pheatmap(Y, cluster_rows = FALSE, cluster_cols = FALSE, 
         color=colorRampPalette(brewer.pal(9,"Blues")[c(1,8)])(30), 
         main = "Adjacency Matrix")[[4]]

p3 <- data.frame(LF1 = X[, 1], LF2 = X[,2], id = LETTERS[1:V]) %>% 
ggplot(aes(x = LF1, y = LF2, label = id)) + 
  geom_text() + 
  theme_minimal() + 
  ggtitle("Latent Space") + 
  xlim(c(-2, 4) ) + 
  ylim(c(-2,4)) + 
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(p1, p2, p3,  nrow = 1)


```
The plots above show the relationship between postion in the latent space (R), 
link connection probability (L), and realized adjacency matrix (C). Note that
data is simulated from a 10-dimensional latent space and so probabilities will
not correspond entirely to distances in the first two dimensions. 

## 1b. Exploring a larger network

Next, we simulate a larger network to which we'll fit our latent space model. 
```{r}
set.seed(1234)

# All hyperparameters and data structures are as above except:
V <- 50 # 100 Number of vertices
H <- 5

# Simulate multiplicative gamma shrinkage process
U <- rep(NA,H)
U[1] <- rgamma(1, a1, 1)
U[2:H] <- rgamma(H-1, a2, 1)

Tau <- rep(NA, H)
for(h in 1:H){
  Tau[h] <- prod(U[1:h])
}

# Simulate latent factors
X <- matrix(NA, nrow = V, ncol = H)
for(h in 1:H){
  X[,h] <- rmvnorm(n = 1, mean = rep(0, V), sigma = diag(Tau[h]^(-1), nrow = V)) 
}

# Simulate intercept
mu <- rnorm(1,mean = mu0, sd = sd.mu) # Normal prior for baseline connection score

# Compute the linear predictor and Z latent connection score
M.Z <- mu + X %*% t(X) 
P <- phi(M.Z)

hist(c(P))
round(quantile(c(P), seq(0,1, 0.1), na.rm = TRUE), 4)

# Sample edges
Y <- matrix(NA, V,V)
Y[upper.tri(Y)] <- rbinom(V*(V-1)/2, 1, P[upper.tri(P)])
Y[lower.tri(Y)] <- t(Y)[lower.tri(Y)]
```
***

### Exercise

Which parameters can you vary? What happens if you change the dimension 
of the latent space? Modify the hyperparameters of the shrinkage process. 
How can you produce a more bimodal vs uniform distribution of latent probabilities?

***

*Answer*
First, we vary the latent factor threshold $H$ while fixing all other hyperparameters. As the number of latent factors increases, the majority are concentrated strongly around 0, however given the increasing number of terms in the sum, the range inner products (and hence the linear predictor) increases; the overall effect on edge probabilities is minor increase in the number of extreme probabilities generated. Note that depending on the degree of shrinkage, all factors over a certain threshold will have a negligible effect. In this example, there is relatively little difference between five and ten latent factors, however with less shrinkage we would expect to see a more pronounced difference.  

Next we vary the shrinkage on the latent factors via the parameters of the $Gamma$ distributions, $a_1, a_2$ while fixing the number of latent factors. Increasing the value of these hyperparameters shrinks the prior variance of the latent factors towards 0 such that they concentrate more strongly around their prior mean, 0. When the latent factor threshold is moderate, this has a dramatic effect on the edge probabilities: with $a_1 = a_2 =2$, the distribution of probabilities is nearly uniform and as we increase $a_1, a_2$ further, the distribution becomes concentrated around relatively high probabilities. $a_1, a_2 \approx 1$ result in less shrinkage and hence larger values of the latent factors
and more polarized values of the inner products. 

```{r, echo = FALSE}
# Define model with BUGS code
fmCode <- nimbleCode({
  
  # Intercept
  mu ~ dnorm(mu0, sd = sd.mu)
  
  # Shrinkage process
  U[1] ~ dgamma(a1, 1)
  for(h in 2:H){
    U[h] ~ dgamma(a2, 1)
  }

  for(h in 1:H){
    Tau[h] <- prod(U[1:h])
  }

  # Latent factors
  for(h in 1:H){
    for(v in 1:V){
      X[v,h] ~ dnorm(M[v] , sd = sqrt(Tau[h]^(-1)))
    }
  }
  
  # Compute linear predictor
  M.Z[1:V,1:V] <- mu + X[,] %*% t(X[,]) # Recall multivariate nodes must be used with []

  # Likelihood
  for (i in 2:V){
    for (j in 1:(i-1)){ # Self-edges not allowed
      P[i,j] <- phi(M.Z[i,j]) # Gaussian CDF 
      P[j,i] <- P[i,j]
      Y[i,j] ~ dbin(size = 1, prob = P[i,j])
    }
  }
  
  
  # Compute logL
  for (i in 2:V){
    for (j in 1:(i-1)){ # Self-edges not allowed
        mat.logL[i,j] <- log((Pi[i,j]^Y[i,j])*((1 - Pi[i,j])^(1-Y[i,j])))
        #mat.logL[j,i] <- 0
    }}
  
  logL <- sum(mat.logL[1:V, 1:V])/2 # diag is zero so this works

})


# Define the constants
diag(Y) <- 0 # diag = NA is useful for plotting only
mu0 = log(mean(Y)/(1-mean(Y))) # prior mean for Mu is logit(meanY)

fmConsts <- list(V = V,
                 H = H, 
                  a1 = a1, a2 = a2,
                  mu0 = mu0, sd.mu = sd.mu,
                  M = rep(0, V))

# Define the data
fmData <- list(Y = Y)

# Set initialization parameters
fmInits <- list(X = matrix(0, V, H), 
                U = 1:H,
                mu = mu0, 
                mat.logL = matrix(0,V,V))

fmDims <- list(Tau = H, X = c(V, H), P = c(V,V), mat.logL = c(V,V))

```

Given the complexity of this model, and the fact that required conjugacies are 
not present in the NIMBLE library, we fit fthis model using Hamiltonian Monte Carlo. 
Since this sampler takes longer to run, we include code to save and read-in the 
model output. Note that in defining our nimbleModel, we now must enable
derivative capabilities in order to fit using HMC. 

```{r, cache = TRUE}
# Define R models
HMCmodel <- nimbleModel(code = fmCode, name = "fm", constants = fmConsts, data = fmData, 
                   dimensions = fmDims, inits = fmInits, buildDerivs = TRUE) # we need to enable derivative capabilities for the model 
HMCmcmc <- buildHMC(HMCmodel, 
                    monitors = c("mu", "logL", "Tau", "P")) # if we don't supply node names, all non-data nodes sampled via HMC
# Compile the C models
Cmodel <- compileNimble(HMCmodel)
Cmcmc <- compileNimble(HMCmcmc, project = HMCmodel)

start.time <- Sys.time()
samplesHMC <- runMCMC(Cmcmc)
Sys.time() - start.time

file_path <- here::here(paste0(save_path, "fmHMC_probit_V50.rds"))
if(save_files){saveRDS(samplesHMC, file_path)}

#samplesHMC <- readRDS(file_path)
```
Now we evaluate performance. It looks quite good, although performance is slow. 
```{r}
# Extract samples and look at them
p.HMC <- samplesHMC[, grepl("P", colnames(samplesHMC))]

# Latent probability recovery: only possible with simualted data
p.post.HMC <- matrix(data = colMeans(p.HMC), byrow = FALSE, nrow = V, ncol = V)
diag(p.post.HMC) <- 0
plot(c(p.post.HMC), c(P))

df <- data.frame(pred = p.post.HMC[upper.tri(p.post.HMC)], y = Y[upper.tri(Y)])
plot.roc(df$y ~ df$pred, percent = TRUE, print.auc = TRUE, main = "ROC Curve - DC-SBM Trade Application")

```

#2. Trade Application


We continue our analysis of the 2021 OECD trade network. We begin with
they same hyperparameters as above.
```{r}

# Read the data
A <- read.csv(here::here(paste0(data_path, "2021Trade/A_subset.csv")))
A <- as.matrix(A)
features <- read.csv(here::here(paste0(data_path, "2021Trade/Features_subset.csv")))

# Define the constants
V <- nrow(A)

mu0 = log(mean(A)/(1-mean(A))) # prior mean for Mu is logit(meanY)

fmConsts <- list(V = V,
                 H = H, 
                  a1 = a1, a2 = a2,
                  mu0 = mu0, sd.mu = sd.mu,
                  M = rep(0, V))

# Define the data
fmData <- list(Y = A)

# Set initialization parameters
fmInits <- list(X = matrix(0, V, H), 
                U = 1:H,
                mu = mu0, 
                mat.logL = matrix(0,V,V))

fmDims <- list(Tau = H, X = c(V, H), P = c(V,V), mat.logL = c(V,V))



```
Now we build the model and run the MCMC. 

```{r, eval = FALSE}
# Define R models
HMCmodel <- nimbleModel(code = fmCode, name = "fm", constants = fmConsts, data = fmData, 
                   dimensions = fmDims, inits = fmInits, buildDerivs = TRUE) # we need to enable derivative capabilities for the model 
HMCmcmc <- buildHMC(HMCmodel, 
                    monitors = c("mu", "logL", "Tau", "P")) # if we don't supply node names, all non-data nodes sampled via HMC
# Compile the C models
Cmodel <- compileNimble(HMCmodel)
Cmcmc <- compileNimble(HMCmcmc, project = HMCmodel)

start.time <- Sys.time()
samplesHMC <- runMCMC(Cmcmc)
Sys.time() - start.time

file_path <- here::here(paste0(save_path, "fmHMC_OECD.rds"))
if(save_files){saveRDS(samplesHMC, file_path)}

#samplesHMC <- readRDS(file_path)
```
Now we evaluate performance. It looks quite good, although performance is slow. 
```{r, eval = FALSE}
# Extract samples and look at them
p.HMC <- samplesHMC[, grepl("P", colnames(samplesHMC))]


# Latent probability recovery: only possible with simualted data
p.post.HMC <- matrix(data = colMeans(p.HMC), byrow = FALSE, nrow = V, ncol = V)
diag(p.post.HMC) <- 0
     
df <- data.frame(pred = p.post.HMC[upper.tri(p.post.HMC)], y = A[upper.tri(A)])
plot.roc(df$y ~ df$pred, percent = TRUE, print.auc = TRUE, main = "ROC Curve - Latent Space Model: Trade Application")


# Mixing
lsamples <- samplesHMC[, grepl("logL", colnames(samplesHMC))]
  mcmc_trace(samplesHMC, regex_pars = c("logL")) 

```
Results above look good but I'm not sure why logL is NA. 

Now try Hoff's package

```{r}

library(eigenmodel)
# Define the model data
reg <- as.factor(features$region) 
X = array(outer(reg,reg, FUN = "==")*1, dim = c(V,V,1)) # x_ij = 1(region_i == region_j)

# Without regressors
fit.eig <- eigenmodel_mcmc(Y = A)

plot(fit.eig)
ulu.post <- fit.eig$ULU_postmean
xb.post <- mean(fit.eig$b_postsamp)*X[,,1]
p.post <- phi(ulu.post + xb.post)

df <- data.frame(pred = p.post[upper.tri(p.post)], y = A[upper.tri(A)])
plot.roc(df$y ~ df$pred, percent = TRUE, print.auc = TRUE, main = "ROC Curve - Hoff Model: Trade Application")

# With regressors
fit.eig.X <- eigenmodel_mcmc(Y=A, X = X)

plot(fit.eig.X)
ulu.post <- fit.eig.X$ULU_postmean
xb.post <- mean(fit.eig.X$b_postsamp)*X[,,1]
p.post <- phi(ulu.post + xb.post)

df <- data.frame(pred = p.post[upper.tri(p.post)], y = A[upper.tri(A)])
plot.roc(df$y ~ df$pred, percent = TRUE, print.auc = TRUE, main = "ROC Curve - Hoff Model +: Trade Application")

```




