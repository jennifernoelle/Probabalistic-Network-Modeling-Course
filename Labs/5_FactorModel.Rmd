---
title: "Latent Factors"
output: html_document
date: "2023-10-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo = FALSE, message = FALSE}
## Set directories
data_path <- 'Data/'
save_path <- 'Results/'

# Load packages
library(tidyverse)
library(here)
library(igraph)
library(ggraph)
library(RColorBrewer)
library(pheatmap)
library(nimble)
library(nimbleHMC)
library(bayesplot)
library(mvtnorm)
library(gridExtra)
library(pROC)
library(latex2exp)

# Do we want to save any output?
save_files <- TRUE

```
# 1. Simulate from a latent space network model 

To begin, we simulate form the latent space network model with a probit link function, 
$\Phi^{-1}$, where $\Phi$ is the cumulative distribution function of the standard normal distribution. 

As described in lecture, this model connection propensity between any two nodes to 
the inner product of their latent features. 

$$p(y_{ij}=1) = \pi_{ij} = \Phi(\alpha + \sum_{h=1}^H \eta_{ih}\eta_{jh})$$
where 

$$
\begin{aligned}
\eta_{ih} &\sim N(0, \tau_h^{-1}) \\
\tau_h &= \prod_{h = 1}^K \vartheta_h \\
\vartheta_1 &\sim \text{Ga}(a_1, 1) \\
\vartheta_h & \sim \text{Ga}(a_2, 1) \quad h = 1, \cdots, K
\end{aligned}
$$

Here, $\alpha$ is an intercept controlling the denseness of connections, 
$\eta_i, \eta_j$ are $H$-dimensional vectors of latent features for nodes $i$ and $j$
respectively, and $\tau$ is a vector of shrinkage factors controlling the influence
of each dimension of the latent space. Note that if $a_2 > 1$, $E[\vartheta_k] > 1$ and so 
$tau_h$ decreases with $h$, providing increasing shrinkage on successive dimensions. 



## 1a. Visualizing a small network 

We begin by simulating a network with only 10 nodes for ease of visualization.

```{r}
set.seed(1234)

# Data (and latent space) dimensions
V <- 10 # Number of vertices
H <- 10 # Dimension of latent space

# Hyperparameters
a1 <- 2.5 # Gamma shrinkage parameter for factor 1
a2 <- 2.5 # Gamma shrinkage parameters for factors 2:H

meanP <- 0.25 # Moderately sparse network
mu0 <- probit(meanP) # Prior mean for intercept
sd.mu <- 0.1 # Prior sd for intercept: consider probit scale

# Simulate multiplicative gamma shrinkage process
U <- rep(NA,H)
U[1] <- rgamma(1, a1, 1)
U[2:H] <- rgamma(H-1, a2, 1)

Tau <- rep(NA, H)
for(h in 1:H){
  Tau[h] <- prod(U[1:h])
}

# Simulate latent factors
X <- matrix(NA, nrow = V, ncol = H)
for(h in 1:H){
  X[,h] <- rmvnorm(n = 1, mean = rep(0, V), sigma = diag(Tau[h]^(-1), nrow = V)) 
}

# Simulate intercept
mu <- rnorm(1,mean = mu0, sd = sd.mu) # Normal prior for baseline connection score

# Compute the linear predictor and Z latent connection score
M.Z <- mu + X %*% t(X) 
P <- phi(M.Z) # Gaussian CDF

# Look at the distribution of connection probabilities
# hist(c(P))
# round(quantile(c(P), seq(0,1, 0.1), na.rm = TRUE), 4)

# Sample edges
Y <- matrix(NA, V,V)
Y[upper.tri(Y)] <- rbinom(V*(V-1)/2, 1, P[upper.tri(P)])
Y[lower.tri(Y)] <- t(Y)[lower.tri(Y)]

diag(Y) <- diag(P) <- NA # Makes plotting more clear

rownames(Y) <- colnames(Y) <- rownames(P) <- colnames(P) <- LETTERS[1:V]

p1 <- pheatmap(P, cluster_rows = FALSE, cluster_cols = FALSE, 
         color=colorRampPalette(brewer.pal(9,"Blues")[c(1,8)])(30), 
         main = "Probability Matrix")[[4]]

p2 <- pheatmap(Y, cluster_rows = FALSE, cluster_cols = FALSE, 
         color=colorRampPalette(brewer.pal(9,"Blues")[c(1,8)])(30), 
         main = "Adjacency Matrix")[[4]]

p3 <- data.frame(LF1 = X[, 1], LF2 = X[,2], id = LETTERS[1:V]) %>% 
ggplot(aes(x = LF1, y = LF2, label = id)) + 
  geom_text() + 
  theme_minimal() + 
  ggtitle("Latent Space") + 
  xlim(c(-2, 4) ) + 
  ylim(c(-2,4)) + 
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(p1, p2, p3,  nrow = 1)


```

The plots above show the relationship between postion in the latent space (R), 
link connection probability (L), and realized adjacency matrix (C). Note that
data is simulated from a 10-dimensional latent space and so probabilities will
not correspond entirely to distances in the first two dimensions. 

## 1b. Exploring a larger network

Next, we simulate a larger network to which we'll fit our latent space model. 
```{r}
set.seed(1234)

# All hyperparameters and data structures are as above except:
V <- 50 # 100 Number of vertices
H <- 5

# Simulate multiplicative gamma shrinkage process
U <- rep(NA,H)
U[1] <- rgamma(1, a1, 1)
U[2:H] <- rgamma(H-1, a2, 1)

Tau <- rep(NA, H)
for(h in 1:H){
  Tau[h] <- prod(U[1:h])
}

# Simulate latent factors
X <- matrix(NA, nrow = V, ncol = H)
for(h in 1:H){
  X[,h] <- rmvnorm(n = 1, mean = rep(0, V), sigma = diag(Tau[h]^(-1), nrow = V)) 
}

# Simulate intercept
mu <- rnorm(1,mean = mu0, sd = sd.mu) # Normal prior for baseline connection score

# Compute the linear predictor and Z latent connection score
M.Z <- mu + X %*% t(X) 
P <- phi(M.Z)

# hist(c(P))
# round(quantile(c(P), seq(0,1, 0.1), na.rm = TRUE), 4)

# Sample edges
Y <- matrix(NA, V,V)
Y[upper.tri(Y)] <- rbinom(V*(V-1)/2, 1, P[upper.tri(P)])
Y[lower.tri(Y)] <- t(Y)[lower.tri(Y)]
```
***

### Exercise

Which parameters can you vary? What happens if you change the dimension 
of the latent space? Modify the hyperparameters of the shrinkage process. 
How can you produce a more bimodal vs uniform distribution of latent probabilities?

***

*Answer*
First, we vary the latent factor threshold $H$ while fixing all other hyperparameters. As the number of latent factors increases, the majority are concentrated strongly around 0, however given the increasing number of terms in the sum, the range inner products (and hence the linear predictor) increases; the overall effect on edge probabilities is minor increase in the number of extreme probabilities generated. Note that depending on the degree of shrinkage, all factors over a certain threshold will have a negligible effect. In this example, there is relatively little difference between five and ten latent factors, however with less shrinkage we would expect to see a more pronounced difference.  

Next we vary the shrinkage on the latent factors via the parameters of the $Gamma$ distributions, $a_1, a_2$ while fixing the number of latent factors. Increasing the value of these hyperparameters shrinks the prior variance of the latent factors towards 0 such that they concentrate more strongly around their prior mean, 0. When the latent factor threshold is moderate, this has a dramatic effect on the edge probabilities: with $a_1 = a_2 =2$, the distribution of probabilities is nearly uniform and as we increase $a_1, a_2$ further, the distribution becomes concentrated around relatively high probabilities. $a_1, a_2 \approx 1$ result in less shrinkage and hence larger values of the latent factors
and more polarized values of the inner products. 

```{r, echo = FALSE}
# Define model with BUGS code
fmCode <- nimbleCode({
  
  # Intercept
  mu ~ dnorm(mu0, sd = sd.mu)
  
  # Shrinkage process
  U[1] ~ dgamma(a1, 1)
  for(h in 2:H){
    U[h] ~ dgamma(a2, 1)
  }

  for(h in 1:H){
    Tau[h] <- prod(U[1:h])
  }

  # Latent factors
  for(h in 1:H){
    for(v in 1:V){
      X[v,h] ~ dnorm(M[v] , sd = sqrt(Tau[h]^(-1)))
    }
  }
  
  # Compute linear predictor
  M.Z[1:V,1:V] <- mu + X[,] %*% t(X[,]) # Recall multivariate nodes must be used with []

  # Likelihood
  for (i in 2:V){
    for (j in 1:(i-1)){ # Self-edges not allowed
      P[i,j] <- phi(M.Z[i,j]) # Gaussian CDF 
      P[j,i] <- P[i,j]
      Y[i,j] ~ dbin(size = 1, prob = P[i,j])
    }
  }

})


# Define the constants
diag(Y) <- 0 # diag = NA is useful for plotting only
mu0 = log(mean(Y)/(1-mean(Y))) # prior mean for Mu is logit(meanY)

fmConsts <- list(V = V,
                 H = H, 
                  a1 = a1, a2 = a2,
                  mu0 = mu0, sd.mu = sd.mu,
                  M = rep(0, V))

# Define the data
fmData <- list(Y = Y)

# Set initialization parameters
fmInits <- list(X = matrix(0, V, H), 
                U = 1:H,
                mu = mu0)

fmDims <- list(Tau = H, X = c(V, H), P = c(V,V))

```

Given the complexity of this model, and the fact that required conjugacies are 
not present in the NIMBLE library, we fit this model using Hamiltonian Monte Carlo. 
Since this sampler takes longer to run, we include code to save and read-in the 
model output. Note that in defining our nimbleModel, we now must enable
derivative capabilities in order to fit using HMC. 

```{r, cache = TRUE}

# Set MCMC args
nchains <- 2
niter <- 5000
nburnin <- 2000

samplesHMC <- nimbleHMC(fmCode, data = fmData, inits = fmInits, constants = fmConsts, 
          monitors = c("P", "mu", "Tau"), WAIC = TRUE, 
          niter = niter, nburnin = nburnin, nchains = nchains)

```

Now we evaluate performance. It looks quite good, although the sampler is slow. 

```{r}
# Extract samples and look at them
all.samples <- do.call(rbind, samplesHMC$samples) # create dataframe with all chains
p.HMC <- all.samples[, grepl("P", colnames(all.samples))]

# Latent probability recovery: only possible with simulated data
p.post.HMC <- matrix(data = colMeans(p.HMC), byrow = FALSE, nrow = V, ncol = V)
diag(p.post.HMC) <- 0

df <- data.frame(p.post = p.post.HMC[upper.tri(p.post.HMC)], p.true = c(P[upper.tri(P)]), y = Y[upper.tri(Y)] )
plot.roc(df$y ~ df$p.post, percent = TRUE, print.auc = TRUE, main = "ROC Curve - Factor Model with Simulated Data")
ggplot(df, aes(x = p.post, y = p.true)) + 
      geom_point(aes(alpha = I(0.3))) + # replace with geom_jitter if probs v similar
      geom_abline(intercept = 0, slope = 1, color = "gray") + 
      xlab(parse(text = TeX('$\\hat{\\pi}_{ij}$'))) + 
      ylab(parse(text = TeX('$\\pi_{ij}$'))) + 
      ggtitle("Factor Model Accuracy on Simulated Data") +
      theme_minimal() + 
      theme(text = element_text(family = "serif", size = 14))
  

# Look at mixing
mu.samples <- matrix(all.samples[, grepl("mu", colnames(all.samples))], nrow(all.samples), 1) 
colnames(mu.samples) <- c("mu")
mcmc_trace(as.matrix(mu.samples))

```

#2. Trade Application


We continue our analysis of the 2021 OECD trade network. We begin with
they same hyperparameters as above.
```{r}

# Read the data
A <- read.csv(here::here(paste0(data_path, "2021Trade/A_subset.csv")))
A <- as.matrix(A)
features <- read.csv(here::here(paste0(data_path, "2021Trade/Features_subset.csv")))

# Define the constants
V <- nrow(A)

mu0 = log(mean(A)/(1-mean(A))) # prior mean for Mu is logit(meanY)

fmConsts <- list(V = V,
                 H = H, 
                  a1 = a1, a2 = a2,
                  mu0 = mu0, sd.mu = sd.mu,
                  M = rep(0, V))

# Define the data
fmData <- list(Y = A)

# Set initialization parameters
fmInits <- list(X = matrix(0, V, H), 
                U = 1:H,
                mu = mu0)

fmDims <- list(Tau = H, X = c(V, H), P = c(V,V))



```
Now we build the model and run the MCMC. 

```{r}
samplesHMC <- nimbleHMC(fmCode, data = fmData, inits = fmInits, constants = fmConsts, 
          monitors = c("P", "mu", "Tau"), WAIC = TRUE, 
          niter = niter, nburnin = nburnin, nchains = nchains)

```
Now we evaluate performance of the latent factor model on our trade data. 
```{r}
# Extract samples and look at them
all.samples <- do.call(rbind, samplesHMC$samples)
p.HMC <- all.samples[, grepl("P", colnames(all.samples))]

# WAIC
samplesHMC$WAIC

# ROC
p.post.HMC <- matrix(data = colMeans(p.HMC), byrow = FALSE, nrow = V, ncol = V)
diag(p.post.HMC) <- 0
     
df <- data.frame(pred = p.post.HMC[upper.tri(p.post.HMC)], y = A[upper.tri(A)])
plot.roc(df$y ~ df$pred, percent = TRUE, print.auc = TRUE, main = "ROC Curve - Latent Space Model: Trade Application")

# Look at mixing
mu.samples <- matrix(all.samples[, grepl("mu", colnames(all.samples))], nrow(all.samples), 1) 
colnames(mu.samples) <- c("mu")
mcmc_trace(as.matrix(mu.samples))


```

While the performance and speed for our sample data sets is very reasonable, Peter Hoff's `eigenmodel` package provides an efficient and straightforward alternative for larger data sets. This package implements the latent eigenmodel described in the paper "Modeling homophily and stochastic equivalence in symmetric
relational data" (Hoff, 2018). 

$$P(y_{i,j} = 1) = \Phi(\mu + \beta^T x_{i,j} + \alpha(u_i, u_j))$$

While this model is very similar to the latent space model presented in class, note that here $\alpha(u_i, u_j) = u_i \lambda u_j$, $u_i \in \mathbb{R}^K$ is a vector of latent characteristics for each node, and $\Lambda$ is a $K \times K$ diagonal matrix with real entries. Note that since the entries of $\Lambda$ can be positive or negative, the model can capture both positive homophily, i.e., similar values of $u_{i,k}$ and $u_{j,k}$ can contribute to or subtract from the edge propensity. Additionally, $x_{i,j}$ provides an array of edge covariates - in our trade application, we can let $x_{i,j} = 1(region_i = region_j)$. 

First, we fit the latent eigenmodel without regressors. 

```{r, cache = TRUE}
library(eigenmodel)
# Define the model data
reg <- as.factor(features$region) 
X = array(outer(reg,reg, FUN = "==")*1, dim = c(V,V,1)) # x_ij = 1(region_i == region_j)

# Without regressors
fit.eig <- eigenmodel_mcmc(Y = A, S = 5000, burn = 2000)

plot(fit.eig)
ulu.post <- fit.eig$ULU_postmean
p.post <- phi(ulu.post)
```

And now we review performance.
```{r}
df <- data.frame(pred = p.post[upper.tri(p.post)], y = A[upper.tri(A)])
plot.roc(df$y ~ df$pred, percent = TRUE, print.auc = TRUE, main = "ROC Curve - Hoff Model: Trade Application")
```

Next, we fit the latent eigenmodel with regressors. 
```{r, cache = TRUE}
# With regressors
fit.eig.X <- eigenmodel_mcmc(Y=A, X = X, S = 5000, burn = 2000)

plot(fit.eig.X)
ulu.post <- fit.eig.X$ULU_postmean
xb.post <- mean(fit.eig.X$b_postsamp)*X[,,1]
p.post <- phi(ulu.post + xb.post)
```

And now we review the performance. 
```{r}
df <- data.frame(pred = p.post[upper.tri(p.post)], y = A[upper.tri(A)])
plot.roc(df$y ~ df$pred, percent = TRUE, print.auc = TRUE, main = "ROC Curve - Hoff Model +: Trade Application")

```




